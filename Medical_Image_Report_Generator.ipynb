{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Medical Image Report Generator.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJ/fMjC54Fpbk+UqPMnyvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M-Tehrani/Image-Caption-Generator/blob/main/Medical_Image_Report_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5UFjTcxu7TV",
        "outputId": "04d37959-da91-468d-e9e6-4b5cedac9f08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'R2Gen'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 89 (delta 13), reused 9 (delta 9), pack-reused 58\u001b[K\n",
            "Unpacking objects: 100% (89/89), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cuhksz-nlp/R2Gen.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd R2Gen/"
      ],
      "metadata": {
        "id": "4d-GSFhOvzn1",
        "outputId": "6f81691e-9130-4b65-8335-81ab7c181b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/R2Gen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from modules.tokenizers import Tokenizer\n",
        "from modules.dataloaders import R2DataLoader\n",
        "from modules.metrics import compute_scores\n",
        "from modules.optimizers import build_optimizer, build_lr_scheduler\n",
        "from modules.trainer import Trainer\n",
        "from modules.loss import compute_loss\n",
        "from models.r2gen import R2GenModel\n",
        "\n",
        "\n",
        "def parse_agrs():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Data input settings\n",
        "    parser.add_argument('--image_dir', type=str, default='data/iu_xray/images/', help='the path to the directory containing the data.')\n",
        "    parser.add_argument('--ann_path', type=str, default='data/iu_xray/annotation.json', help='the path to the directory containing the data.')\n",
        "\n",
        "    # Data loader settings\n",
        "    parser.add_argument('--dataset_name', type=str, default='iu_xray', choices=['iu_xray', 'mimic_cxr'], help='the dataset to be used.')\n",
        "    parser.add_argument('--max_seq_length', type=int, default=60, help='the maximum sequence length of the reports.')\n",
        "    parser.add_argument('--threshold', type=int, default=3, help='the cut off frequency for the words.')\n",
        "    parser.add_argument('--num_workers', type=int, default=2, help='the number of workers for dataloader.')\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='the number of samples for a batch')\n",
        "\n",
        "    # Model settings (for visual extractor)\n",
        "    parser.add_argument('--visual_extractor', type=str, default='resnet101', help='the visual extractor to be used.')\n",
        "    parser.add_argument('--visual_extractor_pretrained', type=bool, default=True, help='whether to load the pretrained visual extractor')\n",
        "\n",
        "    # Model settings (for Transformer)\n",
        "    parser.add_argument('--d_model', type=int, default=512, help='the dimension of Transformer.')\n",
        "    parser.add_argument('--d_ff', type=int, default=512, help='the dimension of FFN.')\n",
        "    parser.add_argument('--d_vf', type=int, default=2048, help='the dimension of the patch features.')\n",
        "    parser.add_argument('--num_heads', type=int, default=8, help='the number of heads in Transformer.')\n",
        "    parser.add_argument('--num_layers', type=int, default=3, help='the number of layers of Transformer.')\n",
        "    parser.add_argument('--dropout', type=float, default=0.1, help='the dropout rate of Transformer.')\n",
        "    parser.add_argument('--logit_layers', type=int, default=1, help='the number of the logit layer.')\n",
        "    parser.add_argument('--bos_idx', type=int, default=0, help='the index of <bos>.')\n",
        "    parser.add_argument('--eos_idx', type=int, default=0, help='the index of <eos>.')\n",
        "    parser.add_argument('--pad_idx', type=int, default=0, help='the index of <pad>.')\n",
        "    parser.add_argument('--use_bn', type=int, default=0, help='whether to use batch normalization.')\n",
        "    parser.add_argument('--drop_prob_lm', type=float, default=0.5, help='the dropout rate of the output layer.')\n",
        "    # for Relational Memory\n",
        "    parser.add_argument('--rm_num_slots', type=int, default=3, help='the number of memory slots.')\n",
        "    parser.add_argument('--rm_num_heads', type=int, default=8, help='the numebr of heads in rm.')\n",
        "    parser.add_argument('--rm_d_model', type=int, default=512, help='the dimension of rm.')\n",
        "\n",
        "    # Sample related\n",
        "    parser.add_argument('--sample_method', type=str, default='beam_search', help='the sample methods to sample a report.')\n",
        "    parser.add_argument('--beam_size', type=int, default=3, help='the beam size when beam searching.')\n",
        "    parser.add_argument('--temperature', type=float, default=1.0, help='the temperature when sampling.')\n",
        "    parser.add_argument('--sample_n', type=int, default=1, help='the sample number per image.')\n",
        "    parser.add_argument('--group_size', type=int, default=1, help='the group size.')\n",
        "    parser.add_argument('--output_logsoftmax', type=int, default=1, help='whether to output the probabilities.')\n",
        "    parser.add_argument('--decoding_constraint', type=int, default=0, help='whether decoding constraint.')\n",
        "    parser.add_argument('--block_trigrams', type=int, default=1, help='whether to use block trigrams.')\n",
        "\n",
        "    # Trainer settings\n",
        "    parser.add_argument('--n_gpu', type=int, default=1, help='the number of gpus to be used.')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='the number of training epochs.')\n",
        "    parser.add_argument('--save_dir', type=str, default='results/iu_xray', help='the patch to save the models.')\n",
        "    parser.add_argument('--record_dir', type=str, default='records/', help='the patch to save the results of experiments')\n",
        "    parser.add_argument('--save_period', type=int, default=1, help='the saving period.')\n",
        "    parser.add_argument('--monitor_mode', type=str, default='max', choices=['min', 'max'], help='whether to max or min the metric.')\n",
        "    parser.add_argument('--monitor_metric', type=str, default='BLEU_4', help='the metric to be monitored.')\n",
        "    parser.add_argument('--early_stop', type=int, default=50, help='the patience of training.')\n",
        "\n",
        "    # Optimization\n",
        "    parser.add_argument('--optim', type=str, default='Adam', help='the type of the optimizer.')\n",
        "    parser.add_argument('--lr_ve', type=float, default=5e-5, help='the learning rate for the visual extractor.')\n",
        "    parser.add_argument('--lr_ed', type=float, default=1e-4, help='the learning rate for the remaining parameters.')\n",
        "    parser.add_argument('--weight_decay', type=float, default=5e-5, help='the weight decay.')\n",
        "    parser.add_argument('--amsgrad', type=bool, default=True, help='.')\n",
        "\n",
        "    # Learning Rate Scheduler\n",
        "    parser.add_argument('--lr_scheduler', type=str, default='StepLR', help='the type of the learning rate scheduler.')\n",
        "    parser.add_argument('--step_size', type=int, default=50, help='the step size of the learning rate scheduler.')\n",
        "    parser.add_argument('--gamma', type=float, default=0.1, help='the gamma of the learning rate scheduler.')\n",
        "\n",
        "    # Others\n",
        "    parser.add_argument('--seed', type=int, default=9233, help='.')\n",
        "    parser.add_argument('--resume', type=str, help='whether to resume the training from existing checkpoints.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    # parse arguments\n",
        "    args = parse_agrs()\n",
        "\n",
        "    # fix random seeds\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(args.seed)\n",
        "\n",
        "    # create tokenizer\n",
        "    tokenizer = Tokenizer(args)\n",
        "\n",
        "    # create data loader\n",
        "    train_dataloader = R2DataLoader(args, tokenizer, split='train', shuffle=True)\n",
        "    val_dataloader = R2DataLoader(args, tokenizer, split='val', shuffle=False)\n",
        "    test_dataloader = R2DataLoader(args, tokenizer, split='test', shuffle=False)\n",
        "\n",
        "    # build model architecture\n",
        "    model = R2GenModel(args, tokenizer)\n",
        "\n",
        "    # get function handles of loss and metrics\n",
        "    criterion = compute_loss\n",
        "    metrics = compute_scores\n",
        "\n",
        "    # build optimizer, learning rate scheduler\n",
        "    optimizer = build_optimizer(args, model)\n",
        "    lr_scheduler = build_lr_scheduler(args, optimizer)\n",
        "\n",
        "    # build trainer and start to train\n",
        "    trainer = Trainer(model, criterion, metrics, optimizer, args, lr_scheduler, train_dataloader, val_dataloader, test_dataloader)\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xbYhMZF0vgGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}